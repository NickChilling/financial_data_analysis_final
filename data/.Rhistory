layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),
batch_size = batch_size,stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:100){
model %>%
fit(x,y,epoch = 1, batch_size =batch_size,
verbose =2,validation_split = 0.2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x)
model <- keras_model_sequential()
batch_size <- 5
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),
batch_size = batch_size,stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:50){
model %>%
fit(x,y,epoch = 1, batch_size =batch_size,
verbose =2,validation_split = 0.2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x)
model <- keras_model_sequential()
batch_size <- 5
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),
batch_size = batch_size,stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:50){
model %>%
fit(x,y,epoch = 1, batch_size =batch_size,
verbose =2,validation_split = 0.2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x)
model <- keras_model_sequential()
batch_size <- 5
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),
batch_size = batch_size,stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:50){
model %>%
fit(x,y,epoch = 1, batch_size =batch_size,
verbose =2,validation_split = 0.2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x,batch_size=batch_size)
model <- keras_model_sequential()
batch_size <- 5
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),
batch_size = batch_size,stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_adam(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:50){
model %>%
fit(x,y,epoch = 1, batch_size =batch_size,
verbose =2,validation_split = 0.2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x,batch_size=batch_size)
View(train_predict)
dataset <- make.dataset(norm_dataset,time_step)
x<-dataset$X
y<-dataset$Y
model <- keras_model_sequential()
batch_size <- 5
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),
batch_size = batch_size,stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_adam(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:50){
model %>%
fit(x,y,epoch = 1, batch_size =batch_size,
verbose =2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x,batch_size=batch_size)
View(train_predict)
mean(abs(y-train_predict)/y)
abs(y-train_predict)/y
sum((y-train_predict)/y)
sum((y-train_predict)/y)
abs(y-train_predict)/y
a = (y-train_predict)/y
abs(a)
sum(abs(a))
abs(a)
for(i in 1:length(a)){}
sum1 = 0
for(i in 1:length(a)){ sum1 = sum1+a[i]}
sum1
max(a)
min(a)
?fit
?fit
model_2 <- keras_model_sequential()  # 2 layer model
model_2 %>%
layer_lstm(units= 5, input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=1)
model2 %>%
compile(loss='mse',optimizer = optimizer_sgd(0.001),
metrics= list("mean_absolute_squared_error")) %>%
fit(x,y,epoch=100, batch_size=batch_size,verbose=2,validation_split=0.2)
train_predict2 <- model2 %>% predict(x)
model_2 <- keras_model_sequential()  # 2 layer model
model_2 %>%
layer_lstm(units= 5, input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=1)
model_2 %>%
compile(loss='mse',optimizer = optimizer_sgd(0.001),
metrics= list("mean_absolute_squared_error")) %>%
fit(x,y,epoch=100, batch_size=batch_size,verbose=2,validation_split=0.2)
train_predict2 <- model_2 %>% predict(x)
model_2 <- keras_model_sequential()  # 2 layer model
model_2 %>%
layer_lstm(units= 5, input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=1)
model_2 %>%
compile(loss='mse',optimizer = optimizer_sgd(0.001),
metrics= list("mean_absolute_percentage_error")) %>%
fit(x,y,epoch=100, batch_size=batch_size,verbose=2,validation_split=0.2)
train_predict2 <- model_2 %>% predict(x)
warnings()
View(train_predict2)
max(train_predict2)
min(train_predict2)
batch_size_3 <- 100
model_3 <- keras_model_sequential()
model_3 %>%
layer_batch_normalization(input_shape=c(time_step,dim(x)[3]),axis = 2) %>%
layer_lstm(units=200, return_sequences=True) %>%
layer_dropout(0.6) %>%
layer_lstm(units=200) %>%
layer_dropout(0.6) %>%
layer_dense(units=1) %>%
summary()
model_3 %>%
compile(loss= 'mse',optimizer = opitimizer_adam(0.001),
metricss = list("mea_absolute_percentage_error")) %>%
fit(x,y,epoch = 100,batch_size = batch_size_3,verbose=2,validation_split=0.2)
train_predict3 <-model_3%>% predict(x)
batch_size_3 <- 100
model_3 <- keras_model_sequential()
model_3 %>%
layer_batch_normalization(input_shape=c(time_step,dim(x)[3]),axis = 2) %>%
layer_lstm(units=200, return_sequences=TRUE) %>%
layer_dropout(0.6) %>%
layer_lstm(units=200) %>%
layer_dropout(0.6) %>%
layer_dense(units=1) %>%
summary()
model_3 %>%
compile(loss= 'mse',optimizer = optimizer_adam(0.001),
metricss = list("mea_absolute_percentage_error")) %>%
fit(x,y,epoch = 100,batch_size = batch_size_3,verbose=2,validation_split=0.2)
train_predict3 <-model_3%>% predict(x)
batch_size_3 <- 100
model_3 <- keras_model_sequential()
model_3 %>%
layer_batch_normalization(input_shape=c(time_step,dim(x)[3]),axis = 2) %>%
layer_lstm(units=200, return_sequences=TRUE) %>%
layer_dropout(0.6) %>%
layer_lstm(units=200) %>%
layer_dropout(0.6) %>%
layer_dense(units=1) %>%
summary()
model_3 %>%
compile(loss= 'mse',optimizer = optimizer_adam(0.001),
metricss = list("mean_absolute_percentage_error")) %>%
fit(x,y,epoch = 100,batch_size = batch_size_3,verbose=2,validation_split=0.2)
train_predict3 <-model_3%>% predict(x)
batch_size_3 <- 100
model_3 <- keras_model_sequential()
model_3 %>%
layer_batch_normalization(input_shape=c(time_step,dim(x)[3]),axis = 2) %>%
layer_lstm(units=200, return_sequences=TRUE) %>%
layer_dropout(0.6) %>%
layer_lstm(units=200) %>%
layer_dropout(0.6) %>%
layer_dense(units=1) %>%
summary()
model_3 %>%
compile(loss= 'mse',optimizer = optimizer_adam(0.001),
metrics = list("mean_absolute_percentage_error")) %>%
fit(x,y,epoch = 100,batch_size = batch_size_3,verbose=2,validation_split=0.2)
train_predict3 <-model_3%>% predict(x)
make.dataset.withoutbn <- function(dataset,time_step){ # input data with dates
numbers = as.matrix(dataset[,2:7]) # 7->8
num_sample = dim(numbers)[1]-time_step-1
num_feature = dim(numbers)[2]
X = array(0,dim = c(num_sample,time_step,num_feature))
Y = array(0,dim = c(num_sample,1)) # close
log_return  = diff(log(dataset[,2]))
for (i in 1:num_sample){
X[i,,] = numbers[i:(i+time_step-1),]
Y[i,] = log_return[(i+time_step)] # 2->7
}
return (list(X=X,Y=Y))
}
trainset <- make.dataset.withoutbn(sz50)
x <- trainset$X
y <- trainset$Y
trainset <- make.dataset.withoutbn(sz50,time_step)
x <- trainset$X
y <- trainset$Y
batch_size_3 <- 100
dim(x)
x[1,1,]
y
x[1,,]
model_3 <- keras_model_sequential()
model_3 %>%
layer_batch_normalization(input_shape=c(time_step,dim(x)[3]),axis = 2) %>%
layer_lstm(units=200, return_sequences=TRUE) %>%
layer_dropout(0.6) %>%
layer_lstm(units=200) %>%
layer_dropout(0.6) %>%
layer_dense(units=1) %>%
summary()
model_3 %>%
compile(loss= 'mse',optimizer = optimizer_adam(0.001),
metrics = list("mean_absolute_percentage_error")) %>%
fit(x,y,epoch = 100,batch_size = batch_size_3,verbose=2,validation_split=0.2)
train_predict3 <-model_3%>% predict(x)
train_predict3 <-model_3%>% predict(x_bn)
x_bn <- trainset_bn$X
y_bn <- trainset_bn$Y
trainset_bn <- make.dataset.withoutbn(sz50,time_step)
x_bn <- trainset_bn$X
y_bn <- trainset_bn$Y
train_predict3 <-model_3%>% predict(x_bn)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5) %>%
layer_dense(units=1)
model_4 %>%
compile(loss='mse',optimizer = optimizer_adam(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict()
train_predict4 <- model_4%>% predict(x_bn)
View(train_predict4)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5,input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=1) %>%
summary()
model_4 %>%
compile(loss='mse',optimizer = optimizer_adam(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5,input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=1) %>%
summary()
model_4 %>%
compile(loss='mse',optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5,input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=5,activation='relu')
layer_dense(units=1) %>%
summary()
model_4 %>%
compile(loss='mean_absolute_percentage_error',optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
View(train_predict4)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5,input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=5,activation='relu')%>%
layer_dense(units=1) %>%
summary()
model_4 %>%
compile(loss='mean_absolute_percentage_error',optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
View(train_predict4)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5,input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=5,activation='relu')%>%
layer_dense(units=1) %>%
summary()
model_4 %>%
compile(loss='mean_absolute_percentage_error',optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5,input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=5,activation='relu')%>%
layer_dense(units=1) %>%
summary()
model_4 %>%
compile(loss='mse',optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
model_4 %>%
compile(loss=mean_absolute_percentage_error,optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
max(y)
min(y)
max(y_bn)
min(y_bn)
plot(y,type='l')
plot(hs[,2])
plot(hs300[,2])
plot(hs300[,2],type='l')
plot(train_predict,type='l')
plot(hs300[,2],type='l')
log_return = diff(log(hs300[,2])
)
plot(log_return,type='l')
plot(log_return[1:500],type='l')
line(train_predict,lty=2 ,col='red')
line(train_predict[1:500],lty=2 ,col='red')
line(train_predict[1:500],)
line(train_predict[1:500])
lines(train_predict[1:500],lty=2 ,col='red')
lines(train_predict2[1:500],lty=2 ,col='bule')
lines(train_predict2[1:500],lty=2 ,col='blue')
acf(train_predict2)
plot(log_return[1:500],type='l')
lines(train_predict2[1:500],lty=2 ,col='blue')
lines(train_predict3[1:500],lty=2,col = 'red')
lines(train_predict4[1:500],lty=2,col = 'green')
# 数据可视化, 模型对比
plot(y,type='l')
lines(train_predict,lty=2,col ='green')
lines(train_predict,lty=2,col = 'blue')
lines(train_predict,lty=2,col ='green')
lines(train_predict2,lty=2,col = 'blue')
lines(train_predict3,lty=2,col = 'yellow')
lines(train_predict4,lty=2,col = 'red')
?plot
# 数据可视化, 模型对比
plot(x=hs300[1,],y=y,type='l', main='nmsl')
# 数据可视化, 模型对比
plot(x=hs300[,1],y=y,type='l', main='nmsl')
# 数据可视化, 模型对比
plot(y=y,type='l', main='nmsl')
# 数据可视化, 模型对比
plot(y,type='l', main='nmsl')
?legend
legend()
legend('a')
legend(legend = '1')
legend('a',legend = '1')
# 数据可视化, 模型对比
plot(y,type='l', main='各模型预测值对比',xlab='time series',ylab = 'log return')
lines(train_predict,lty=2,col ='green')
lines(train_predict2,lty=2,col = 'blue')
lines(train_predict3,lty=2,col = 'yellow')
lines(train_predict4,lty=2,col = 'red')
make.graph  <- function(x,y){
ggplot(mapping = aes(x=x,y=y))+geom_line()+geom_point()+theme_economist()+scale_color_economist()
}
make.graph(sz50[,1],sz50[,2])
# data exploration
plot(hs300[2,],type = 'l')
# data exploration
plot(hs300[,2],type = 'l')
# data exploration
plot(hs300[,2],type = 'l',xlab='time series',ylab = 'price',main= '沪深300线')
# data exploration
plot(hs300[,1],hs300[,2],type = 'l',xlab='time series',ylab = 'price',main= '沪深300线')
# data exploration
plot(hs300[,1],hs300[,2],type = 'l',xlab='time series',ylab = 'price',main= '沪深300线')
hs300[,1]
hs300[1,1]
5/12
# data exploration
ts = seq(1,len(hs300))/144+2009.4
# data exploration
ts = seq(1,length(hs300))/144+2009.4
plot(ts,hs300[,2],type = 'l',xlab='time series',ylab = 'price',main= '沪深300线')
length(ts)
# data exploration
ts = seq(1,dim(hs300)[1])/144+2009.4
plot(ts,hs300[,2],type = 'l',xlab='time series',ylab = 'price',main= '沪深300线')
hs300[1,1]
dim(hs300)[1]/10
ts = seq(1,dim(hs300)[1])/244+2009.4
plot(ts,hs300[,2],type = 'l',xlab='time series',ylab = 'index',main= '沪深300线')
lg_r = diff(log(hs300[,2]))
plot(ts,hs300[,2],type='l',xlab= 'time series', ylab = 'log return',main='沪深300 对数收益率')
plot(ts_diff,lg_r,type='l',xlab= 'time series', ylab = 'log return',main='沪深300 对数收益率')
ts_diff = seq(1,dim(hs300)[1]-1)
lg_r = diff(log(hs300[,2]))
plot(ts_diff,lg_r,type='l',xlab= 'time series', ylab = 'log return',main='沪深300 对数收益率')
model_2 <- keras_model_sequential()  # 2 layer model
model_2 %>%
layer_lstm(units= 5, input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=1)%>%
summary()
model_2 %>%
compile(loss='mse',optimizer = optimizer_sgd(0.001),
metrics= list("mean_absolute_percentage_error")) %>%
fit(x,y,epoch=100, batch_size=batch_size,verbose=2,validation_split=0.2)
train_predict2 <- model_2 %>% predict(x)
install.packages(c("forecast", "tseries"))
setwd("c:/Users/zanzh/OneDrive/研一下/金融数据分析/LSTM_VS_ARIMA/data/")
library(tseries)
library(forecast)
da1 = read.csv("上证50.csv", header = T)
head(da1)
logSZ50Pri = log((da1[c(168: 2167), 3]))
head(logSZ50Pri)
tdx = c(1: 2000)/250 + 2010
plot(tdx, logSZ50Pri, xlab = 'day', ylab = 'ln(price)', type='l')  #TODO 输出的图放在graph/下面
title(main = '上证50指数变化')
dlogSZ50Pri = diff(logSZ50Pri) #做一阶差分
adf.test(dlogSZ50Pri) # 在5%的显著水平下拒绝原假设，一阶差分后数据平稳  # TODO adf检验的结果保存为文本放到graph/下面
par(mfcol = c(1, 2))
acf(dlogSZ50Pri, lag = 20) #TODO 同上
pacf(dlogSZ50Pri, lag = 20)
# 观察数据ar使用3阶，ma采用两阶，建立arima(3, 0, 2)模型
SZ50_model = arima(dlogSZ50Pri, order = c(3, 0, 2), include.mean = F)
SZ50_model
Box.test(SZ50_model$residuals) # p值显著大于0.05，残差为白噪声序列，选取的模型合适，不需要使用garch模型  #TODO 检验结果保存
#计算原本数据的对数收益率
logr = diff(log(da1[c(2168:2468), 3]))
# 短期预测
SZ50_arima_forecast = forecast(SZ50_model, h = 250)
head(logr)
short_di_sum = 0
for(i in 1: 5) {
short_di_sum = short_di_sum + abs(SZ50_arima_forecast[[4]][i] - logr[i])
}
short_di_sum
# 中期预测
middle_di_sum = 0
for(i in 1: 60) {
middle_di_sum = middle_di_sum + abs(SZ50_arima_forecast[[4]][i] - logr[i])
}
middle_di_sum
# 长期预测
long_di_sum = 0
for(i in 1: 250) {
long_di_sum = long_di_sum + abs(SZ50_arima_forecast[[4]][i] - logr[i])
}
long_di_sum
#TODO 画图看一下, 更直观
# 画图样例
plot(logr,type= 'l',main = "comparison", xlab = 'time series', ylab='log return') # 实际值
lines(SZ50_arima_forecast[[4]], col= 'red') # 预测值
SZ50_arima_forecast[[4]]
acf(SZ50_arima_forecast)
acf(SZ50_arima_forecast[[4]])
