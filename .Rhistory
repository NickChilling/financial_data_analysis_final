model %>%
fit(x,y,epoch = 100, batch_size =100,
verbose =2,validation_split = 0.2)
train_predict = model %>% predict(x)
model <- keras_model_sequential()
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:100){
model %>%
fit(x,y,epoch = 1, batch_size =100,
verbose =2,validation_split = 0.2,shuffle = FALSE)
model %>$
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x)
model <- keras_model_sequential()
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:100){
model %>%
fit(x,y,epoch = 1, batch_size =100,
verbose =2,validation_split = 0.2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x)
dataset <- make.dataset(norm_dataset,time_step)
x<-dataset$X
y<-dataset$Y
dim(x)
model <- keras_model_sequential()
batch_size <- 80
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),
batch_size = batch_size,stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:100){
model %>%
fit(x,y,epoch = 1, batch_size =batch_size,
verbose =2,validation_split = 0.2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x)
model <- keras_model_sequential()
batch_size <- 5
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),
batch_size = batch_size,stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:50){
model %>%
fit(x,y,epoch = 1, batch_size =batch_size,
verbose =2,validation_split = 0.2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x)
model <- keras_model_sequential()
batch_size <- 5
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),
batch_size = batch_size,stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:50){
model %>%
fit(x,y,epoch = 1, batch_size =batch_size,
verbose =2,validation_split = 0.2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x)
model <- keras_model_sequential()
batch_size <- 5
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),
batch_size = batch_size,stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:50){
model %>%
fit(x,y,epoch = 1, batch_size =batch_size,
verbose =2,validation_split = 0.2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x,batch_size=batch_size)
model <- keras_model_sequential()
batch_size <- 5
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),
batch_size = batch_size,stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_adam(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:50){
model %>%
fit(x,y,epoch = 1, batch_size =batch_size,
verbose =2,validation_split = 0.2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x,batch_size=batch_size)
View(train_predict)
dataset <- make.dataset(norm_dataset,time_step)
x<-dataset$X
y<-dataset$Y
model <- keras_model_sequential()
batch_size <- 5
model %>%
layer_lstm(units= 10,input_shape = c(time_step,dim(x)[3]),
batch_size = batch_size,stateful = TRUE) %>%
layer_dense(units=1)
summary(model)
model %>%
compile(
loss = 'mse',
optimizer = optimizer_adam(0.001),
metrics = list('mean_absolute_percentage_error') )
for (i in 1:50){
model %>%
fit(x,y,epoch = 1, batch_size =batch_size,
verbose =2,shuffle = FALSE)
model %>%
reset_states()
}
# layer_batch_normalization(input_shape = c(time_step,dim(x)[3]),axis=2)
train_predict = model %>% predict(x,batch_size=batch_size)
View(train_predict)
mean(abs(y-train_predict)/y)
abs(y-train_predict)/y
sum((y-train_predict)/y)
sum((y-train_predict)/y)
abs(y-train_predict)/y
a = (y-train_predict)/y
abs(a)
sum(abs(a))
abs(a)
for(i in 1:length(a)){}
sum1 = 0
for(i in 1:length(a)){ sum1 = sum1+a[i]}
sum1
max(a)
min(a)
?fit
?fit
model_2 <- keras_model_sequential()  # 2 layer model
model_2 %>%
layer_lstm(units= 5, input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=1)
model2 %>%
compile(loss='mse',optimizer = optimizer_sgd(0.001),
metrics= list("mean_absolute_squared_error")) %>%
fit(x,y,epoch=100, batch_size=batch_size,verbose=2,validation_split=0.2)
train_predict2 <- model2 %>% predict(x)
model_2 <- keras_model_sequential()  # 2 layer model
model_2 %>%
layer_lstm(units= 5, input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=1)
model_2 %>%
compile(loss='mse',optimizer = optimizer_sgd(0.001),
metrics= list("mean_absolute_squared_error")) %>%
fit(x,y,epoch=100, batch_size=batch_size,verbose=2,validation_split=0.2)
train_predict2 <- model_2 %>% predict(x)
model_2 <- keras_model_sequential()  # 2 layer model
model_2 %>%
layer_lstm(units= 5, input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=1)
model_2 %>%
compile(loss='mse',optimizer = optimizer_sgd(0.001),
metrics= list("mean_absolute_percentage_error")) %>%
fit(x,y,epoch=100, batch_size=batch_size,verbose=2,validation_split=0.2)
train_predict2 <- model_2 %>% predict(x)
warnings()
View(train_predict2)
max(train_predict2)
min(train_predict2)
batch_size_3 <- 100
model_3 <- keras_model_sequential()
model_3 %>%
layer_batch_normalization(input_shape=c(time_step,dim(x)[3]),axis = 2) %>%
layer_lstm(units=200, return_sequences=True) %>%
layer_dropout(0.6) %>%
layer_lstm(units=200) %>%
layer_dropout(0.6) %>%
layer_dense(units=1) %>%
summary()
model_3 %>%
compile(loss= 'mse',optimizer = opitimizer_adam(0.001),
metricss = list("mea_absolute_percentage_error")) %>%
fit(x,y,epoch = 100,batch_size = batch_size_3,verbose=2,validation_split=0.2)
train_predict3 <-model_3%>% predict(x)
batch_size_3 <- 100
model_3 <- keras_model_sequential()
model_3 %>%
layer_batch_normalization(input_shape=c(time_step,dim(x)[3]),axis = 2) %>%
layer_lstm(units=200, return_sequences=TRUE) %>%
layer_dropout(0.6) %>%
layer_lstm(units=200) %>%
layer_dropout(0.6) %>%
layer_dense(units=1) %>%
summary()
model_3 %>%
compile(loss= 'mse',optimizer = optimizer_adam(0.001),
metricss = list("mea_absolute_percentage_error")) %>%
fit(x,y,epoch = 100,batch_size = batch_size_3,verbose=2,validation_split=0.2)
train_predict3 <-model_3%>% predict(x)
batch_size_3 <- 100
model_3 <- keras_model_sequential()
model_3 %>%
layer_batch_normalization(input_shape=c(time_step,dim(x)[3]),axis = 2) %>%
layer_lstm(units=200, return_sequences=TRUE) %>%
layer_dropout(0.6) %>%
layer_lstm(units=200) %>%
layer_dropout(0.6) %>%
layer_dense(units=1) %>%
summary()
model_3 %>%
compile(loss= 'mse',optimizer = optimizer_adam(0.001),
metricss = list("mean_absolute_percentage_error")) %>%
fit(x,y,epoch = 100,batch_size = batch_size_3,verbose=2,validation_split=0.2)
train_predict3 <-model_3%>% predict(x)
batch_size_3 <- 100
model_3 <- keras_model_sequential()
model_3 %>%
layer_batch_normalization(input_shape=c(time_step,dim(x)[3]),axis = 2) %>%
layer_lstm(units=200, return_sequences=TRUE) %>%
layer_dropout(0.6) %>%
layer_lstm(units=200) %>%
layer_dropout(0.6) %>%
layer_dense(units=1) %>%
summary()
model_3 %>%
compile(loss= 'mse',optimizer = optimizer_adam(0.001),
metrics = list("mean_absolute_percentage_error")) %>%
fit(x,y,epoch = 100,batch_size = batch_size_3,verbose=2,validation_split=0.2)
train_predict3 <-model_3%>% predict(x)
make.dataset.withoutbn <- function(dataset,time_step){ # input data with dates
numbers = as.matrix(dataset[,2:7]) # 7->8
num_sample = dim(numbers)[1]-time_step-1
num_feature = dim(numbers)[2]
X = array(0,dim = c(num_sample,time_step,num_feature))
Y = array(0,dim = c(num_sample,1)) # close
log_return  = diff(log(dataset[,2]))
for (i in 1:num_sample){
X[i,,] = numbers[i:(i+time_step-1),]
Y[i,] = log_return[(i+time_step)] # 2->7
}
return (list(X=X,Y=Y))
}
trainset <- make.dataset.withoutbn(sz50)
x <- trainset$X
y <- trainset$Y
trainset <- make.dataset.withoutbn(sz50,time_step)
x <- trainset$X
y <- trainset$Y
batch_size_3 <- 100
dim(x)
x[1,1,]
y
x[1,,]
model_3 <- keras_model_sequential()
model_3 %>%
layer_batch_normalization(input_shape=c(time_step,dim(x)[3]),axis = 2) %>%
layer_lstm(units=200, return_sequences=TRUE) %>%
layer_dropout(0.6) %>%
layer_lstm(units=200) %>%
layer_dropout(0.6) %>%
layer_dense(units=1) %>%
summary()
model_3 %>%
compile(loss= 'mse',optimizer = optimizer_adam(0.001),
metrics = list("mean_absolute_percentage_error")) %>%
fit(x,y,epoch = 100,batch_size = batch_size_3,verbose=2,validation_split=0.2)
train_predict3 <-model_3%>% predict(x)
train_predict3 <-model_3%>% predict(x_bn)
x_bn <- trainset_bn$X
y_bn <- trainset_bn$Y
trainset_bn <- make.dataset.withoutbn(sz50,time_step)
x_bn <- trainset_bn$X
y_bn <- trainset_bn$Y
train_predict3 <-model_3%>% predict(x_bn)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5) %>%
layer_dense(units=1)
model_4 %>%
compile(loss='mse',optimizer = optimizer_adam(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict()
train_predict4 <- model_4%>% predict(x_bn)
View(train_predict4)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5,input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=1) %>%
summary()
model_4 %>%
compile(loss='mse',optimizer = optimizer_adam(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5,input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=1) %>%
summary()
model_4 %>%
compile(loss='mse',optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5,input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=5,activation='relu')
layer_dense(units=1) %>%
summary()
model_4 %>%
compile(loss='mean_absolute_percentage_error',optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
View(train_predict4)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5,input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=5,activation='relu')%>%
layer_dense(units=1) %>%
summary()
model_4 %>%
compile(loss='mean_absolute_percentage_error',optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
View(train_predict4)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5,input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=5,activation='relu')%>%
layer_dense(units=1) %>%
summary()
model_4 %>%
compile(loss='mean_absolute_percentage_error',optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
model_4 <- keras_model_sequential()
model_4 %>%
layer_lstm(units = 5,input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=5,activation='relu')%>%
layer_dense(units=1) %>%
summary()
model_4 %>%
compile(loss='mse',optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
model_4 %>%
compile(loss=mean_absolute_percentage_error,optimizer = optimizer_sgd(0.001),
metrics = list('mean_absolute_percentage_error')) %>%
fit(x_bn,y_bn,epoch = 100,batch_size = batch_size_3,verbose =2,validation_split=0.2)
train_predict4 <- model_4%>% predict(x_bn)
max(y)
min(y)
max(y_bn)
min(y_bn)
plot(y,type='l')
plot(hs[,2])
plot(hs300[,2])
plot(hs300[,2],type='l')
plot(train_predict,type='l')
plot(hs300[,2],type='l')
log_return = diff(log(hs300[,2])
)
plot(log_return,type='l')
plot(log_return[1:500],type='l')
line(train_predict,lty=2 ,col='red')
line(train_predict[1:500],lty=2 ,col='red')
line(train_predict[1:500],)
line(train_predict[1:500])
lines(train_predict[1:500],lty=2 ,col='red')
lines(train_predict2[1:500],lty=2 ,col='bule')
lines(train_predict2[1:500],lty=2 ,col='blue')
acf(train_predict2)
plot(log_return[1:500],type='l')
lines(train_predict2[1:500],lty=2 ,col='blue')
lines(train_predict3[1:500],lty=2,col = 'red')
lines(train_predict4[1:500],lty=2,col = 'green')
# 数据可视化, 模型对比
plot(y,type='l')
lines(train_predict,lty=2,col ='green')
lines(train_predict,lty=2,col = 'blue')
lines(train_predict,lty=2,col ='green')
lines(train_predict2,lty=2,col = 'blue')
lines(train_predict3,lty=2,col = 'yellow')
lines(train_predict4,lty=2,col = 'red')
?plot
# 数据可视化, 模型对比
plot(x=hs300[1,],y=y,type='l', main='nmsl')
# 数据可视化, 模型对比
plot(x=hs300[,1],y=y,type='l', main='nmsl')
# 数据可视化, 模型对比
plot(y=y,type='l', main='nmsl')
# 数据可视化, 模型对比
plot(y,type='l', main='nmsl')
?legend
legend()
legend('a')
legend(legend = '1')
legend('a',legend = '1')
# 数据可视化, 模型对比
plot(y,type='l', main='各模型预测值对比',xlab='time series',ylab = 'log return')
lines(train_predict,lty=2,col ='green')
lines(train_predict2,lty=2,col = 'blue')
lines(train_predict3,lty=2,col = 'yellow')
lines(train_predict4,lty=2,col = 'red')
make.graph  <- function(x,y){
ggplot(mapping = aes(x=x,y=y))+geom_line()+geom_point()+theme_economist()+scale_color_economist()
}
make.graph(sz50[,1],sz50[,2])
# data exploration
plot(hs300[2,],type = 'l')
# data exploration
plot(hs300[,2],type = 'l')
# data exploration
plot(hs300[,2],type = 'l',xlab='time series',ylab = 'price',main= '沪深300线')
# data exploration
plot(hs300[,1],hs300[,2],type = 'l',xlab='time series',ylab = 'price',main= '沪深300线')
# data exploration
plot(hs300[,1],hs300[,2],type = 'l',xlab='time series',ylab = 'price',main= '沪深300线')
hs300[,1]
hs300[1,1]
5/12
# data exploration
ts = seq(1,len(hs300))/144+2009.4
# data exploration
ts = seq(1,length(hs300))/144+2009.4
plot(ts,hs300[,2],type = 'l',xlab='time series',ylab = 'price',main= '沪深300线')
length(ts)
# data exploration
ts = seq(1,dim(hs300)[1])/144+2009.4
plot(ts,hs300[,2],type = 'l',xlab='time series',ylab = 'price',main= '沪深300线')
hs300[1,1]
dim(hs300)[1]/10
ts = seq(1,dim(hs300)[1])/244+2009.4
plot(ts,hs300[,2],type = 'l',xlab='time series',ylab = 'index',main= '沪深300线')
lg_r = diff(log(hs300[,2]))
plot(ts,hs300[,2],type='l',xlab= 'time series', ylab = 'log return',main='沪深300 对数收益率')
plot(ts_diff,lg_r,type='l',xlab= 'time series', ylab = 'log return',main='沪深300 对数收益率')
ts_diff = seq(1,dim(hs300)[1]-1)
lg_r = diff(log(hs300[,2]))
plot(ts_diff,lg_r,type='l',xlab= 'time series', ylab = 'log return',main='沪深300 对数收益率')
model_2 <- keras_model_sequential()  # 2 layer model
model_2 %>%
layer_lstm(units= 5, input_shape =c(time_step,dim(x)[3])) %>%
layer_dense(units=1)%>%
summary()
model_2 %>%
compile(loss='mse',optimizer = optimizer_sgd(0.001),
metrics= list("mean_absolute_percentage_error")) %>%
fit(x,y,epoch=100, batch_size=batch_size,verbose=2,validation_split=0.2)
train_predict2 <- model_2 %>% predict(x)
